{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPmARPXPC1vTm0UBxw7Hixy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"yLvaMfZtcheZ"},"outputs":[],"source":["!pip install lxml\n","!pip install requests\n","!pip install beautifulsoup4"]},{"cell_type":"code","source":["import requests\n","import csv\n","from bs4 import BeautifulSoup\n","from google.colab import drive"],"metadata":{"id":"VYJlC0udcqGg","executionInfo":{"status":"ok","timestamp":1683784281186,"user_tz":-300,"elapsed":445,"user":{"displayName":"Nikita Okolelov","userId":"14645599043367864818"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["drive.mount('/content/drive')\n","files = \"drive/MyDrive/Parser/products_for_site_new.csv\"\n","file_new = \"drive/MyDrive/Parser/products_for_site.csv\"\n"],"metadata":{"id":"SMkcWQS1lpwO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(files, mode=\"w\", encoding='utf-8') as w_file:\n","    names = [\"id\", \"link\"]\n","    file_writer = csv.DictWriter(w_file, delimiter = \";\", \n","                                 lineterminator=\"\\r\", fieldnames=names)\n","    file_writer.writeheader()\n","    order = 0\n","    for k in range(1, 1500):\n","      print(k)\n","      url = 'https://productcenter.ru/producers/r-chieliabinskaia-obl-224/page-' + str(k)\n","      print(url)\n","      response = requests.get(url)\n","      soup = BeautifulSoup(response.text, 'lxml')\n","      items = soup.find_all('div', class_='ci_main')\n","\n","      for n, i in enumerate(items, start=1):\n","          order += 1\n","          file_writer.writerow({\"link\": i.find('a', class_='link').get('href'), \"id\": order})\n","        \n"],"metadata":{"id":"FarP8zmwmHQV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(files, encoding='utf-8') as r_file:\n","    file_reader = csv.reader(r_file, delimiter = \";\")\n","    for row in file_reader:\n","      print(row[1])"],"metadata":{"id":"Pbk34SjUkZT6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["arr = [0 for i in range(10)]\n","print(arr)"],"metadata":{"id":"d0gzqrMR00_t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(file_new, encoding='utf-8') as r_file:\n","    file_reader = csv.reader(r_file, delimiter = \";\")\n","    f = open('drive/MyDrive/Parser/company.txt', 'w')\n","    id = -1\n","    for row in file_reader:  \n","        f.write(str(row[0]) + ';' + row[1] + ';' + row[2] + ';' + str(row[3]) + ';' + str(row[4]) + ';' + str(row[5]) + ';' + str(row[6]) + ';' + str(row[7]) + ';' + str(row[8]) + ';' + str(row[9]) + ';' + str(row[10]) + ';' + str(row[11]) + ';' + str(row[12]) + '\\n')\n","          "],"metadata":{"id":"yzQGZLkdrOeD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(file_new, encoding='utf-8') as r_file:\n","    file_reader = csv.reader(r_file, delimiter = \";\")\n","    with open(files, mode=\"w\", encoding='utf-8') as w_file:\n","      names = [\"id\", \"url\", \"name\", \"ur_name\", \"ogrn\", \"inn\", \"kpp\", \"ur_address\", \"phone\", \"email\", \"site\", \"address\" \"product1\", \"product2\", \"product3\", \"product4\", \"product5\", \"product6\", \"product7\", \"product8\", \"product9\", \"product10\"]\n","      file_writer = csv.DictWriter(w_file, delimiter = \";\", \n","                                 lineterminator=\"\\r\", fieldnames=names)\n","      file_writer.writeheader()\n","      id = -1\n","      for row in file_reader:\n","        # count = 0\n","        if (row[1] != \"link\"):\n","          url = row[1]\n","          print(url)\n","          response = requests.get(url)\n","          soup = BeautifulSoup(response.text, 'lxml')\n","          item = soup.find_all('div', class_='contact_information')\n","          id = row[0]\n","          name = row[2]\n","          product1 = row[3]\n","          product2 = row[4]\n","          product3 = row[5]\n","          product4 = row[6]\n","          product5 = row[7]\n","          product6 = row[8]\n","          product7 = row[9]\n","          product8 = row[10]\n","          product9 = row[11]\n","          product10 = row[12]\n","\n","          for n, i in enumerate(item, start=1):\n","              # ur_name = i.find('h1', class_='').text.strip()\n","              company_data = i.find('table', class_='company_data').text.strip()\n","              # phone = i.find('td', itemprop='address').text.strip()\n","              # email = i.find('td', itemprop='address').text.strip()\n","              # site = i.find('td', itemprop='address').text.strip()\n","              address = i.find('td', itemprop='address').text.strip()\n","              # print(ur_name)\n","              print(company_data.split())\n","              print(address)\n","\n","          # items = soup.find_all('div', class_='card_item product')\n","          # itemsAgain = soup.find_all('div', class_='card_item product vip')\n","\n","          # for k, j in enumerate(items, start=1):\n","          #     itemName = j.find('a', class_='link').text.strip()\n","          #     arr[k - 1] = itemName\n","          #     # print(j.find('a', class_='link').text.strip())\n","          #     # itemPrice = j.find('div', class_='item_price').text.strip()\n","          #     # count += 1\n","          # for s, d in enumerate(itemsAgain, start=1):\n","          #     itemName = d.find('a', class_='link').text.strip()\n","          #     arr[s - 1] = itemName\n","          #     # itemPrice = d.find('div', class_='item_price').text.strip()\n","          #     # count += 1\n","          #file_writer.writerow({ \"id\": id, \"url\": url, \"name\": Name, \"product1\": arr[0], \"product2\": arr[1], \"product3\": arr[2], \"product4\": arr[3], \"product5\": arr[4], \"product6\": arr[5], \"product7\": arr[6], \"product8\": arr[7], \"product9\": arr[8], \"product10\": arr[9]})\n","\n","print(\"успешно\")"],"metadata":{"id":"7vpZVlibPk67"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with open(files, encoding='utf-8') as r_file:\n","    file_reader = csv.reader(r_file, delimiter = \";\")\n","    with open(file_new, mode=\"w\", encoding='utf-8') as w_file:\n","      names = [\"id\", \"url\", \"name\", \"product1\", \"product2\", \"product3\", \"product4\", \"product5\", \"product6\", \"product7\", \"product8\", \"product9\", \"product10\"]\n","      file_writer = csv.DictWriter(w_file, delimiter = \";\", \n","                                 lineterminator=\"\\r\", fieldnames=names)\n","      file_writer.writeheader()\n","      id = -1\n","      for row in file_reader:\n","        arr = [0 for i in range(10)]\n","        id += 1\n","        # count = 0\n","        if (row[1] != \"link\"):\n","          url = 'https://productcenter.ru' + row[1]\n","          print(url)\n","          response = requests.get(url)\n","          soup = BeautifulSoup(response.text, 'lxml')\n","          item = soup.find_all('div', class_='iv_content')\n","\n","          for n, i in enumerate(item, start=1):\n","              Name = i.find('h1', class_='').text.strip()\n","\n","          items = soup.find_all('div', class_='card_item product')\n","          itemsAgain = soup.find_all('div', class_='card_item product vip')\n","\n","          for k, j in enumerate(items, start=1):\n","              itemName = j.find('a', class_='link').text.strip()\n","              arr[k - 1] = itemName\n","          for s, d in enumerate(itemsAgain, start=1):\n","              itemName = d.find('a', class_='link').text.strip()\n","              arr[s - 1] = itemName\n","          file_writer.writerow({ \"id\": id, \"url\": url, \"name\": Name, \"product1\": arr[0], \"product2\": arr[1], \"product3\": arr[2], \"product4\": arr[3], \"product5\": arr[4], \"product6\": arr[5], \"product7\": arr[6], \"product8\": arr[7], \"product9\": arr[8], \"product10\": arr[9]})\n","\n","print(\"успешно\")\n"],"metadata":{"id":"7GVrIZ7VzTJc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["file_products = \"drive/MyDrive/Parser/file_products.csv\"\n","with open(files, encoding='utf-8') as r_file:\n","    file_reader = csv.reader(r_file, delimiter = \";\")\n","    with open(file_products, mode=\"w\", encoding='utf-8') as w_file:\n","      names = [\"name\", \"price\", \"att\", \"maker\", \"comments\", \"category\"]\n","      file_writer = csv.DictWriter(w_file, delimiter = \",\", \n","                                 lineterminator=\"\\r\", fieldnames=names)\n","      file_writer.writeheader()\n","      id = -1\n","      for row in file_reader:\n","        arr = [0 for i in range(10)]\n","        id += 1\n","        # count = 0\n","        if (row[1] != \"link\"):\n","          url = 'https://productcenter.ru' + row[1]\n","          print(url)\n","          response = requests.get(url)\n","          soup = BeautifulSoup(response.text, 'lxml')\n","          item = soup.find_all('div', class_='iv_content')\n","          items = soup.find_all('div', class_='card_item product')\n","          itemsAgain = soup.find_all('div', class_='card_item product vip')\n","\n","          for k, i in enumerate(item, start=1):\n","              Name = i.find('h1', class_='').text.strip()\n","              print(Name)\n","        \n","          for k, j in enumerate(items, start=1):\n","              itemName = j.find('a', class_='link').text.strip()\n","              price = j.find('b', class_='')\n","              if price:\n","                  price = price.text.strip()\n","              else:\n","                  price = 0\n","              print(itemName)\n","              print(price)\n","              print(Name)\n","              file_writer.writerow({\"name\": itemName, \"price\": price, \"att\": \"Производители\", \"maker\": Name, \"comments\": 0, \"category\": \"Misc\"})\n","\n","          for s, d in enumerate(itemsAgain, start=1):\n","              itemName = d.find('a', class_='link').text.strip()\n","              price = j.find('b', class_='')\n","              if price:\n","                  price = price.text.strip()\n","              else:\n","                  price = 0\n","              print(itemName)\n","              print(price)\n","              print(Name)\n","              file_writer.writerow({\"name\": itemName, \"price\": price, \"att\": \"Производители\", \"maker\": Name, \"comments\": 0, \"category\": \"Misc\"})\n","\n","          \n","\n","print(\"успешно\")"],"metadata":{"id":"Dv6Cu1bs_L05"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(arr)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AjdG7xFxy_5N","executionInfo":{"status":"ok","timestamp":1657989321742,"user_tz":-300,"elapsed":272,"user":{"displayName":"Nikita Okolelov","userId":"14645599043367864818"}},"outputId":"2bde5d7a-7f9b-4ab2-8364-05737485fc92"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['Межкомнатные двери «Silvia»', 'Межкомнатные двери «Элеганс»', 'Межкомнатные двери «Principiano»', 'Межкомнатные двери «Арт-Лайн» продолжение', 'Межкомнатные двери «Арт-Лайн»', 'Межкомнатные двери «Лайт»', 'Межкомнатные двери «Сканди Люкс» продолжение', 'Межкомнатные двери «Бьянка»', 'Межкомнатные двери «Сканди Люкс»', 'Межкомнатные двери «Selena»']\n"]}]},{"cell_type":"code","source":["!pip3 uninstall keras-nightly\n","!pip3 uninstall -y tensorflow\n","!pip3 install keras==2.1.6\n","!pip3 install tensorflow==1.15.0\n","!pip3 install h5py==2.10.0"],"metadata":{"id":"ill-46PHaORO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install pymystem3"],"metadata":{"id":"vxgjgqorc12O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from nltk.stem import SnowballStemmer\n","from nltk.tokenize import word_tokenize\n","import requests\n","import csv\n","from bs4 import BeautifulSoup\n","from google.colab import drive\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')"],"metadata":{"id":"Hnw9_I7PlbSp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**`Удаление окончаний в словах`**"],"metadata":{"id":"f-HaZYxQlKaK"}},{"cell_type":"code","source":["def deleteEndWord(text):\n","  snowball = SnowballStemmer(language=\"russian\")\n","  tokenizer_word = word_tokenize(text)\n","  new_word = []\n","  for word in tokenizer_word:\n","    new_word.append(snowball.stem(word))\n","  complete_word = ' '.join(new_word)\n","  return complete_word"],"metadata":{"id":"L6JmRrC5lJ9q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**`Подготовка стоп слов`**"],"metadata":{"id":"BQQZW0hWo1ms"}},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","stop_words = stopwords.words(\"russian\")\n","stop_words.append('«')\n","stop_words.append('»')\n","stop_words.append('-')\n","stop_words.append('псто-')\n","stop_words.append('.')\n","stop_words.append('пр')"],"metadata":{"id":"kZ3tJaxko5bE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**`Удаление стоп слов`**"],"metadata":{"id":"tHvKxW-4oh6C"}},{"cell_type":"code","source":["def deleteStopWords(text):\n","  words = text.split()\n","  filter_words = []\n","  for word in words:\n","    if word not in stop_words:\n","      filter_words.append(word)\n","  new_words = ' '.join(filter_words)\n","  return new_words"],"metadata":{"id":"kaaDcqeWohal"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**`Создание вектора слоав`**"],"metadata":{"id":"bweJBzRorlX8"}},{"cell_type":"code","source":["def wordOrder(text):\n","  order = []\n","  words = text.split()\n","  print(words)\n","  for word in words:\n","    try:\n","      order.append(word_indexes[word])\n","    except KeyError:\n","      order.append(-1)\n","  return order"],"metadata":{"id":"kHRrt4xzrprs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**`Подготовка слов для токенизации`**"],"metadata":{"id":"v-jcfxnqi1Z5"}},{"cell_type":"code","source":["text = []\n","with open(files, encoding='utf-8') as r_file:\n","    file_reader = csv.reader(r_file, delimiter = \";\")\n","    for row in file_reader:\n","      if (row[1] != \"link\"):\n","        url = 'https://productcenter.ru' + row[1]\n","        response = requests.get(url)\n","        soup = BeautifulSoup(response.text, 'lxml')\n","\n","        items = soup.find_all('div', class_='card_item product ')\n","        itemsAgain = soup.find_all('div', class_='card_item product vip')\n","\n","        for k, j in enumerate(items, start=1):\n","          itemName = j.find('a', class_='link').text.strip()\n","          text.append(deleteStopWords(deleteEndWord(itemName)))\n","        for s, d in enumerate(itemsAgain, start=1):\n","          itemName = d.find('a', class_='link').text.strip()\n","          text.append(deleteStopWords(deleteEndWord(itemName)))\n","\n","print(text)"],"metadata":{"id":"AmLMUc24iUaA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**`Сохранение спарсенных слов`**"],"metadata":{"id":"dCTjqsSPnOqb"}},{"cell_type":"code","source":["file_for_word = \"drive/MyDrive/Parser/words_for_tokenizer.cvs\"\n","with open(file_for_word, mode=\"w\", encoding='utf-8') as w_file:\n","    names = [\"id\", \"words\"]\n","    file_writer = csv.DictWriter(w_file, delimiter = \";\", \n","                                 lineterminator=\"\\r\", fieldnames=names)\n","    file_writer.writeheader()\n","    i = 0\n","    for word in text:\n","      i += 1\n","      file_writer.writerow({'id': i, 'words': word})"],"metadata":{"id":"XktbowP5nTmK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**`Токенизация слов`**"],"metadata":{"id":"YBZ742fri7PA"}},{"cell_type":"code","source":["tokenizer = Tokenizer(num_words=100)\n","\n","tokenizer.fit_on_texts(text)\n","word_indexes = tokenizer.word_index\n","print(word_indexes)"],"metadata":{"id":"bHL394aIWBA2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**`Сохраненние токенизированных слов`**"],"metadata":{"id":"9cuY3BTOxngE"}},{"cell_type":"code","source":["tokenized_words = \"drive/MyDrive/Parser/tokenized_words.cvs\"\n","with open(tokenized_words, mode=\"w\", encoding='utf-8') as w_file:\n","    names = [\"token\", \"word\"]\n","    file_writer = csv.DictWriter(w_file, delimiter = \";\", \n","                                 lineterminator=\"\\r\", fieldnames=names)\n","    file_writer.writeheader()\n","    for item in word_indexes.items():\n","      file_writer.writerow({'token': item[1], 'word': item[0]})"],"metadata":{"id":"k78wiJJJtArl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**`Создание файла с токенизированными словами`**\n","\n"],"metadata":{"id":"sBrIl7WgqZt8"}},{"cell_type":"code","source":["with open(files, encoding='utf-8') as r_file:\n","    file_reader = csv.reader(r_file, delimiter = \";\")\n","    with open(file_new, mode=\"w\", encoding='utf-8') as w_file:\n","      names = [\"id\", \"products\"]\n","      file_writer = csv.DictWriter(w_file, delimiter = \";\", \n","                                 lineterminator=\"\\r\", fieldnames=names)\n","      file_writer.writeheader()\n","      for row in file_reader:\n","        arr = []\n","        # count = 0\n","        if (row[1] != \"link\"):\n","          url = 'https://productcenter.ru' + row[1]\n","          response = requests.get(url)\n","          soup = BeautifulSoup(response.text, 'lxml')\n","\n","          items = soup.find_all('div', class_='card_item product ')\n","          itemsAgain = soup.find_all('div', class_='card_item product vip')\n","\n","          for k, j in enumerate(items, start=1):\n","              itemName = j.find('a', class_='link').text.strip()\n","              filter_words = deleteStopWords(deleteEndWord(itemName))          \n","              filter_words = wordOrder(filter_words)\n","              for word in filter_words:\n","                arr.append(word)\n","\n","              \n","\n","          for s, d in enumerate(itemsAgain, start=1):\n","              itemName = d.find('a', class_='link').text.strip()\n","              filter_words = deleteStopWords(deleteEndWord(itemName))\n","              filter_words = wordOrder(filter_words)\n","              for word in filter_words:\n","                arr.append(word)\n","\n","          file_writer.writerow({\"id\": row[0], \"products\": arr})\n","\n","                \n","\n","print(\"успешно\")"],"metadata":{"id":"bP3nXKODqz8r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Тех. часть."],"metadata":{"id":"oOX8A8iFnIU7"}},{"cell_type":"code","source":["import nltk\n","nltk.download('stopwords')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WzIDis4Amh7K","executionInfo":{"status":"ok","timestamp":1658756675737,"user_tz":-300,"elapsed":465,"user":{"displayName":"Nikita Okolelov","userId":"14645599043367864818"}},"outputId":"b8a29ae6-773c-452d-a07d-e9e934db7d62"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":59}]},{"cell_type":"code","source":["\n","\n","words1 = \"« Я » и ты мы когда будем Насос\"\n","words1 = words1.split()\n","a = []\n","print(stop_words)\n","for word in words1:\n","  if word not in stop_words:\n","    a.append(word)\n","print(a)"],"metadata":{"id":"FshAcCS5mkOB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Добавление окончаний\n"],"metadata":{"id":"cWSDYT-v1vD3"}},{"cell_type":"code","source":["ends = ['']"],"metadata":{"id":"dQWIDRmc1yIt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import nltk\n","nltk.download('averaged_perceptron_tagger')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-udkZ8BR9O6A","executionInfo":{"status":"ok","timestamp":1680103244083,"user_tz":-300,"elapsed":369,"user":{"displayName":"Nikita Okolelov","userId":"14645599043367864818"}},"outputId":"991a114e-0e24-402b-8ae4-24f2f123ab67"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["def remove_adj(sentence):\n","    \n","    stop_tags = [\"JJ\", \"JJR\", \"JJS\", \"NN\", \"»\"]\n","    tokens = nltk.word_tokenize(sentence)\n","    tags = list(reversed(nltk.pos_tag(tokens)))\n","    noun_located = False\n","    stop_reached = False\n","    final_sent = ''\n","\n","    for word,pos in tags:\n","        if noun_located == False and pos == 'NN':\n","            noun_located = True\n","            final_sent+=f' {word}'\n","        elif stop_reached == False and pos in stop_tags:\n","            stop_reached = True\n","        elif stop_reached == True:\n","            final_sent+=f' {word}'\n","\n","    final_sent = ' '.join(reversed(final_sent.split(' ')))      \n","    return final_sent"],"metadata":{"id":"FXFhkIqx8i8V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["f_w = \"drive/MyDrive/Parser/test.csv\"\n","f_r = \"drive/MyDrive/Parser/products.cvs\" #разрешение поменяй\n","\n","with open(f_r, encoding='utf-8') as r_file:\n","    file_reader = csv.reader(r_file, delimiter = \";\")\n","    with open(f_w, mode=\"w\", encoding='utf-8') as w_file:\n","      names = [\"product\"]\n","      file_writer = csv.DictWriter(w_file, delimiter = \";\", \n","                                 lineterminator=\"\\r\", fieldnames=names)\n","      file_writer.writeheader()\n","      for row in file_reader:\n","        if row[2] != \"product\":\n","          print(\"С прилагательными: \"  + row[2])\n","          print(\"С прилагательных: \"  + remove_adj(row[2]))\n","          print()"],"metadata":{"id":"qo9zk6GY7Ogo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["file_products = \"drive/MyDrive/Parser/file_products.csv\"\n","new_file_products = \"drive/MyDrive/Parser/new_file_products.csv\"\n","with open(file_products, encoding='utf-8') as r_file:\n","    file_reader = csv.reader(r_file, delimiter = \",\")\n","    with open(new_file_products, mode=\"w\", encoding='utf-8') as w_file:\n","      names = [\"name\", \"price\", \"att\", \"maker\", \"view_att\", \"isGlobal_att\", \"comments\", \"category\"]\n","      file_writer = csv.DictWriter(w_file, delimiter = \",\", \n","                                 lineterminator=\"\\r\", fieldnames=names)\n","      file_writer.writeheader()\n","      for row in file_reader:\n","          if row[0] != \"name\":\n","              print(\"Name: \" + row[0] + \" Price: \" + row[1] + \"P Производитель: \" + row[3])\n","              file_writer.writerow({\"name\": row[0], \"price\": row[1], \"att\": \"Производитель\", \"maker\": row[3], \"view_att\": 1, \"isGlobal_att\": 1, \"comments\": 0, \"category\": \"Misc\"})\n","\n","          \n","\n","print(\"успешно\")"],"metadata":{"id":"WXKfYk6hYvCx"},"execution_count":null,"outputs":[]}]}